{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Hello Team!\n\nI hope at least a few of your read this, I've created a [Discord Server](https://discord.gg/xEFYjWQ) for discussion. \n\nThis is my first attempt, currently data loading is not working so it is entirely untested."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\n\n# plotting\nimport pandas_profiling\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='white', context='notebook', palette='deep')\n\n# models\nfrom xgboost import XGBClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting # Gives us additional classifiers\nfrom sklearn.ensemble import (RandomForestClassifier,\n                              AdaBoostClassifier,\n                              GradientBoostingClassifier,\n                              ExtraTreesClassifier,\n                              BaggingClassifier,\n                              ExtraTreesClassifier,\n                              VotingClassifier,\n                              HistGradientBoostingClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\n\n# metrics and selection\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import (GridSearchCV, \n                                     KFold, \n                                     train_test_split, \n                                     cross_val_score, \n                                     StratifiedKFold, \n                                     learning_curve)\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"train = pd.read_csv('../input/learn-together/train.csv')\ntest = pd.read_csv('../input/learn-together/test.csv')\nid_test = test['Id']\n\ntrain_len = len(train)\ndataset = pd.concat(objs=[train,test], axis=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature analysis will go here, but curently I can't see the data :/","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modeling\n\nrandom_state = 0 \nkfold = StratifiedKFold(n_splits=10)\n\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\n\ncv_results = []\nfor classifier in classifiers:\n    cv_results.append(cross_val_score(classifier, X_train, y=Y_train, scoring=\"accuracy\", cv = kfold, n_jobs=4))\n    \ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv.result.mean())\n    cv_std.append(cv.result.std())\n    \n    \ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparams w GridSearchCV (did I mention no data?)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# curve code by Ghouzam, he has awesome kernels\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",X_train,Y_train,cv=kfold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_RFC = pd.Series(RFC_best.predict(test), name=\"RFC\")\ntest_ExtC = pd.Series(ExtC_best.predict(test), name=\"ExtC\")\ntest_SVMC = pd.Series(SVMC_best.predict(test), name=\"SVC\")\ntest_AdaC = pd.Series(ada_best.predict(test), name=\"Ada\")\ntest_GBC = pd.Series(GBC_best.predict(test), name=\"GBC\")\n\n\n# Concatenate all classifier results\nensemble_results = pd.concat([test_Survived_RFC,test_Survived_ExtC,test_Survived_AdaC,test_Survived_GBC, test_Survived_SVMC],axis=1)\n\n\ng= sns.heatmap(ensemble_results.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vote = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n\nvote = vote.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = pd.Series(votingC.predict(test), name=\"Cover Type\")\n\nresults = pd.concat([IDtest,test_pred],axis=1)\n\nresults.to_csv(\"ensemble_python_voting.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}